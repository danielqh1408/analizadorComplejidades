; ======================================================================
; Configuration File for the Complexity Analyzer Project
; ======================================================================
; This file centralizes settings for environment, hardware, performance,
; APIs, and logging. Parse using Python's configparser.

[ENVIRONMENT]
; Define the application's operating mode: development, testing, production
mode = development
; Enable debug features (e.g., more verbose errors)
debug_mode = true

[PATHS]
; Define root paths based on the repository structure.
; All paths are relative to the project root (ANALIZADORCOMPLEJIDADES/).
root_dir = .
source_dir = src/
modules_dir = src/modules/
services_dir = src/services/
external_dir = src/external/
architecture_docs = Architecture/
data_dir = data/
test_data = data/tests/
example_data = data/examples/
; Output directories (ensure 'logs' and 'output' are in .gitignore)
log_dir = logs/
output_dir = output/

[HARDWARE]
; Hardware-specific optimizations for the Ryzen 7 7700X and RTX 4060 Ti
; Enable CUDA acceleration via CuPy/Numba if available
use_gpu = true
; Specify the default device ID for CUDA
gpu_device_id = 0
; Default number of CPU workers for multiprocessing/joblib.
; (16 threads on Ryzen 7 7700X)
cpu_workers = 16
; Set to 0 or -1 to use all available cores (Joblib convention)
; cpu_workers = -1

[PERFORMANCE]
; Fine-tuning for performance-critical libraries
; Enable Numba JIT compilation (numba==0.58.1)
use_numba_jit = true
; Enable Numba's parallel execution for @njit(parallel=True)
numba_parallel = true
; Set the number of threads Numba should use
numba_num_threads = 16
; Caching mechanism for Numba (recommended)
numba_caching = true
; Joblib backend (multiprocessing is default, 'loky' is robust)
joblib_backend = loky

[LOGGING]
; Configuration for the system logger (src/services/logger.py)
log_file = logs/analyzer.log
; Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level = DEBUG
; Format for log messages
log_format = %(asctime)s - [%(levelname)s] - %(name)s (%(filename)s:%(lineno)d) - %(message)s
; Enable logging to console
log_to_console = true
; Enable logging to file
log_to_file = true

[API]
; Configuration for the FastAPI server (src/main.py)
host = 127.0.0.1
port = 8000
; Number of Uvicorn workers (use 1 for development)
workers = 3

[LLM]
; Configuration for the LLM Assistant (src/external/llm_assistant.py)
; Specify the primary LLM provider: 'gemini' or 'openai'
primary_provider = gemini

; Google Gemini Settings
gemini_model = gemini-pro
; (gemini-1.5-pro, gemini-1.5-flash, etc.)
gemini_api_base_url = https://generativelace.googleapis.com/v1beta/models/


[SECURITY]
; API Key Management
; DO NOT HARDCODE API KEYS HERE.
; Load keys from environment variables (e.DE .env file).
gemini_api_key_env = GEMINI_API_KEY