âš™ï¸ Complexity Analyzer â€” Execution & Validation Guide
ğŸ“¦ 1. Environment Setup

Before running or testing the system, make sure you have the proper environment configured.

ğŸ§° Requirements

Python: 3.11 (64-bit)

Pip: latest version

GPU: NVIDIA RTX 4060 Ti (CUDA 12.x)

CPU: AMD Ryzen 7 7700X (8 cores / 16 threads)

RAM: DDR5 (32 GB recommended)

ğŸ§© Dependencies

All required libraries are defined in requirements.txt.

Install them with:

pip install -r requirements.txt

ğŸ§  Verify installation

Check Python and package versions:

python --version
pip show numpy numba sympy fastapi uvicorn


Expected output (approximate):

Python 3.11.x
numpy 1.26.4
numba 0.58.1
sympy 1.12
fastapi 0.115.0
uvicorn 0.30.1

ğŸ—‚ï¸ 2. Project Structure
complexity-analyzer/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ lexer.py
â”‚   â”‚   â”œâ”€â”€ parser.py
â”‚   â”‚   â”œâ”€â”€ ast_nodes.py
â”‚   â”‚   â”œâ”€â”€ analyzer.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ config_loader.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ file_io.py
â”‚   â””â”€â”€ orchestrator/
â”‚       â”œâ”€â”€ orchestrator.py
â”‚       â””â”€â”€ llm_wrapper.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_lexer_parser.py
â”‚   â”œâ”€â”€ test_analyzer.py
â”‚   â”œâ”€â”€ test_main_api.py
â”‚   â””â”€â”€ test_metrics.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ developer_guide.md
â”‚   â”œâ”€â”€ execution.md
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.ini
â””â”€â”€ README.md

ğŸš€ 3. Running the Application
ğŸ§© Step 1 â€” Load Environment Variables

Before starting, make sure your LLM API keys are set as environment variables:

setx GEMINI_API_KEY "your_api_key_here"


(Use export instead of setx on Linux/Mac.)

ğŸ§© Step 2 â€” Launch the Backend API

Run the backend using Uvicorn:

uvicorn src.main:app --reload --host 127.0.0.1 --port 8000


Expected console output:

INFO:     Uvicorn running on http://127.0.0.1:8000
INFO:     Application startup complete.

ğŸ§© Step 3 â€” Test the API Endpoint

Use curl, Postman, or your browser to send a request:

curl -X POST "http://127.0.0.1:8000/api/analyze" ^
     -H "Content-Type: application/json" ^
     -d "{\"type\": \"pseudocode\", \"content\": \"FOR i ğŸ¡¨ 1 TO n DO sum ğŸ¡¨ sum + i END FOR\"}"


Expected JSON response:

{
  "status": "ok",
  "type": "pseudocode",
  "complexity": { "O": "O(n)", "Î©": "Î©(n)", "Î˜": "Î˜(n)" },
  "execution_time_ms": 12.3,
  "llm_validation": "Matches theoretical complexity"
}

ğŸ§ª 4. Running Unit Tests

All tests are located under /tests and can be executed with pytest.

ğŸ”¹ Run all tests:
pytest -v

ğŸ”¹ Run a specific test:
pytest tests/test_analyzer.py -v

ğŸ”¹ Generate coverage report:
pytest --cov=src --cov-report=term-missing


Expected example:

---------- coverage: platform win32, python 3.11 ----------
Name                               Stmts   Miss  Cover
--------------------------------------------------------
src/modules/analyzer.py               92      4    95%
src/modules/parser.py                 85      3    96%
src/services/logger.py                47      0   100%
--------------------------------------------------------
TOTAL                                327      7    97%

ğŸ“Š 5. Verifying System Components
âœ… 5.1. Lexer

Command:

python -m src.modules.lexer tests/examples/simple_for.pseudo


Expected:

[INFO] Lexer completed â€” 14 tokens

âœ… 5.2. Parser

Command:

pytest tests/test_parser.py -v


Expected:

Parser correctly builds AST with 1 root node and 2 nested loops.

âœ… 5.3. Analyzer

Command:

pytest tests/test_analyzer.py -v


Expected:

Detected complexity: O(n log n), Î©(n), Î˜(n log n)

âœ… 5.4. LLM Integration (Optional)

Command:

python src/orchestrator/llm_wrapper.py --test


Expected:

LLM connection established â€” Gemini response: valid

ğŸ” 6. Logs & Metrics
ğŸ“ Logs

All runtime logs are stored in:

logs/app.log


Typical log entry:

[2025-11-01 14:22:04] [INFO] [Analyzer]: Completed O/Î©/Î˜ evaluation in 12.5 ms

ğŸ“Š Metrics

Generated metrics (time, tokens, latency) are stored in:

logs/metrics.json


To view metrics in console:

python src/services/metrics.py --view

ğŸ§® 7. GPU & Performance Validation

To validate that your GPU and multithreading optimizations are working:

ğŸ§© GPU computation test
python -m src.modules.analyzer --gpu-test


Expected:

GPU acceleration (CUDA 12.x) active â€” execution speed: 1.82x faster

ğŸ§© CPU multithreading test
python -m src.modules.analyzer --cpu-test


Expected:

CPU cores utilized: 16 â€” average analysis time: 8.2 ms

ğŸ§  8. Troubleshooting
Issue	Cause	Solution
ModuleNotFoundError	Wrong project root	Run commands from the projectâ€™s root directory.
LLM API error	Invalid or missing key	Check environment variables (GEMINI_API_KEY / OPENAI_API_KEY).
Test import error	Wrong Python path	Add project root to PYTHONPATH.
Slow execution	CPU mode only	Check if CUDA driver is installed.
No logs appear	Config path issue	Verify config.ini [LOGGING] section.
ğŸ§© 9. Full Validation Checklist

âœ… Lexer recognizes all pseudocode structures
âœ… Parser builds a valid AST hierarchy
âœ… Analyzer computes O, Î©, Î˜ accurately
âœ… Metrics and logs recorded successfully
âœ… API endpoint /api/analyze responds correctly
âœ… Tests (pytest) all pass with >90% coverage
âœ… GPU acceleration validated
âœ… Configuration and logging functional

ğŸ 10. Clean Shutdown

When finished testing:

CTRL + C


Then clear any compiled or cache files:

python -m compileall -b src
rmdir /s /q __pycache__

âœ… Summary

This guide provides the complete procedure for:

Installing dependencies

Running the backend API

Executing local unit tests

Validating CPU/GPU performance

Checking logs and metrics

Once all validations pass, the system is considered ready for CI/CD integration and production deployment.